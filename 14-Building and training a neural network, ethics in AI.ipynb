{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training a CNN\n",
    "Today we're going to take the datasets we created yesterday and train a convolutional neural network (CNN) on them. Since this directly depends on the output files from yesterday, take a moment to copy and paste your output files into today's directory. Again, let's take a second to remember our two guiding questions:\n",
    "\n",
    "- _What_ is in an image (e.g. debris, buildings, etc.)?\n",
    "- _Where_ are these things located _in 3D space_ ?\n",
    "\n",
    "Recall that we are posing the first question as a classification problem (e.g. does this image have flooding or not), and we are deploying a CNN to aid in the classification.\n",
    "\n",
    "As before, this is heavily based off of the following tutorials: https://github.com/LADI-Dataset/ladi-tutorial/blob/master/Tutorials/Pytorch_Data_Load.md, https://github.com/LADI-Dataset/ladi-tutorial/blob/master/Tutorials/Train_Test_Classifier.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "Yesterday we developed simple machine learning models using sci-kit learn. While the package is good for quick and dirty machine learning models, we are going to need much more powerful tools to take on an image classification task of the kind we're seeing. To that end, we're going to be working with PyTorch, typically considered as the state of the art in deep learning. It was developed primarily by Facebook AI in and released for Python in 2017. There are two main advantages of PyTorch vs. many other Python packages:\n",
    "- A fairly high level interface for building neural networks\n",
    "- Parallelization support via GPU\n",
    "While it is one of the easier ways to build a neural network, there is still a fairly steep learning curve. Thus, today will be all about learning how to implement in PyTorch (and less about what's going on under the hood). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from torchvision import transforms, utils\n",
    "except:\n",
    "    !conda install --yes torchvision --no-channel-priority\n",
    "    from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# You want to change these to be your own filenames\n",
    "csv_file = 'flood_tiny_metadata.csv'\n",
    "label_csv = 'flood_tiny_label.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataset from your csv files\n",
    "The first step to using PyTorch is creating a dataset. All datasets in PyTorch need to be a subclass of torch.utils.data.Dataset, which is the general class that represents a dataset. When creating your dataset, you usually need to overwrite two methods:\n",
    "- ```__len__```, so that when you call len(dataset) it returns the number of images\n",
    "- ```__getitem__```, so that when you call dataset[i] it returns the ith item. \n",
    "As usual, we're also going to define ```__init__``` to take in the arguments we care about.\n",
    "\n",
    "We also need to decide what we want a sample of our dataset to look like. Ours is going to look like a dictionary as follows: ```{'image': image, 'image_name': img_name, 'damage:flood/water': label, uuid': uuid, 'timestamp': timestamp, 'gps_lat': gps_lat, 'gps_lon': gps_lon, 'gps_alt': gps_alt, 'orig_file_size': file_size, 'orig_width': width, 'orig_height': height}```. Of course, there's no requirement that it be a dictionary, though it should somehow incorporate the input and output (in our case, the image and the label). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenient function for showing the images\n",
    "def show_image(image):\n",
    "    plt.imshow(image)\n",
    "    # pause a bit so that plots are updated\n",
    "    plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_url_to_local_path(url):\n",
    "    '''\n",
    "    gets the location of the image in the shared directory so we don't have to redownload\n",
    "    '''\n",
    "    return '/home/jovyan/course/ladi/'+'/'.join(url.split('/')[3:])\n",
    "\n",
    "class FloodTinyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, label_csv, transform = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with metadata.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.flood_tiny_metadata = pd.read_csv(csv_file)\n",
    "        # get the path in the shared directory\n",
    "        self.flood_tiny_metadata['local_path'] = self.flood_tiny_metadata['url'].apply(convert_url_to_local_path)\n",
    "        self.flood_tiny_label = pd.read_csv(label_csv)\n",
    "        self.flood_tiny_data = pd.merge(self.flood_tiny_metadata, \n",
    "                                        self.flood_tiny_label,\n",
    "                                       on=\"s3_path\")\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.flood_tiny_metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        ## Load images from shared directory. There is no need to download images to local machine. ##\n",
    "        local_path = self.flood_tiny_metadata.iloc[idx]['local_path']\n",
    "        url = self.flood_tiny_metadata.iloc[idx]['url']\n",
    "        try:\n",
    "            image = Image.fromarray(io.imread(local_path))\n",
    "            img_name = local_path\n",
    "        except:\n",
    "            image = Image.fromarray(io.imread(url))\n",
    "            img_name = url\n",
    "        uuid = self.flood_tiny_data.iloc[idx, 1]\n",
    "        timestamp = self.flood_tiny_data.iloc[idx, 2]\n",
    "        gps_lat = self.flood_tiny_data.iloc[idx, 3]\n",
    "        gps_lon = self.flood_tiny_data.iloc[idx, 4]\n",
    "        gps_alt = self.flood_tiny_data.iloc[idx, 5]\n",
    "        file_size = self.flood_tiny_data.iloc[idx, 6]\n",
    "        width = self.flood_tiny_data.iloc[idx, 7]\n",
    "        height = self.flood_tiny_data.iloc[idx, 8]\n",
    "        label = self.flood_tiny_data.iloc[idx, -1]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = {'image': image, 'image_name': img_name, 'damage:flood/water': label, 'uuid': uuid, 'timestamp': timestamp, 'gps_lat': gps_lat, 'gps_lon': gps_lon, 'gps_alt': gps_alt, 'orig_file_size': file_size, 'orig_width': width, 'orig_height': height}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a quick sanity check\n",
    "flood_tiny_dataset = FloodTinyDataset(csv_file = csv_file, label_csv = label_csv)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(flood_tiny_dataset)):\n",
    "    sample = flood_tiny_dataset[i]\n",
    "\n",
    "    print(i, \n",
    "          sample['damage:flood/water'], \n",
    "          sample['image_name'], \n",
    "          sample['uuid'], \n",
    "          sample['timestamp'], \n",
    "          sample['gps_lat'], \n",
    "          sample['gps_lon'], \n",
    "          sample['gps_alt'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.title('Sample #{}'.format(i))\n",
    "    show_image(sample['image'])\n",
    "\n",
    "    if i == 5:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "There are now two things to take care of:\n",
    "- Neural networks usually expect all images to be of the same size, but sometimes datasets are not all of the same size. The LADI dataset includes image of different sizes. \n",
    "- Like we saw in our thought exercise last class, there is a huge amount of variability. Because of this, the top performing CNN's usually train on hundreds of thousands of images, much more than what we have! We're going to have to somehow increase the number of images available to us. \n",
    "\n",
    "One solution is to augment the dataset by *transforming* it. Most transformations are actually fairly straightforward.\n",
    "\n",
    "- ```Resize```: to resize the input PIL Image to the given size.\n",
    "- ```RandomCrop```: to crop from image randomly. This is data augmentation.\n",
    "- ```RandomRotation```: to rotate the image by angle.\n",
    "- ```RandomHorizontalFlip```: to horizontally flip the given PIL Image randomly with a given probability.\n",
    "- ```ToTensor```: to convert the numpy images to torch images (we need to swap axes).\n",
    "\n",
    "These functions each have their own set of arguments, which can be referenced here(https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
    "\n",
    "Now let's try to apply these transforms, first individually and then all at once. The key to effective transforms is to have \"enough\" transformation that the input image is visually distinct from the output image, but not so much that the output image is no longer representative of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = transforms.Resize(768)\n",
    "crop = transforms.RandomCrop(512)\n",
    "rotate = transforms.RandomRotation(20)\n",
    "flip_demo = transforms.RandomHorizontalFlip(1) # flip with 100% chance just to demo\n",
    "flip = transforms.RandomHorizontalFlip(p=0.5)\n",
    "composed = transforms.Compose([scale,\n",
    "                               crop,\n",
    "                               rotate,\n",
    "                               flip_demo])\n",
    "\n",
    "# Apply each of the above transforms on sample.\n",
    "fig = plt.figure()\n",
    "sample = flood_tiny_dataset[198]\n",
    "for i, tsfrm in enumerate([scale, crop, rotate, flip_demo, composed]):\n",
    "    transformed_image = tsfrm(sample['image'])\n",
    "\n",
    "    ax = plt.subplot(2, 3, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title(type(tsfrm).__name__)\n",
    "    show_image(transformed_image)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "Compared to simple ```for``` loop to iterate over data, ```torch.utils.data.DataLoader``` is an iterator which provides more features:\n",
    "\n",
    "- Batching the data.\n",
    "- Shuffling the data.\n",
    "- Load the data in parallel using multiprocessing workers.\n",
    "\n",
    "In the previous section, three transforms are performed on a sample. In this section, users can learn to use ```DataLoader``` to transform all images in the dataset.\n",
    "\n",
    "First, a new dataset with transform needs to be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = FloodTinyDataset(csv_file = csv_file, \n",
    "                                       label_csv = label_csv, \n",
    "                                       transform = transforms.Compose([scale, \n",
    "                                                                       crop, \n",
    "                                                                       rotate, \n",
    "                                                                       flip, \n",
    "                                                                       transforms.ToTensor()]\n",
    "                                                                     )\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to show a batch\n",
    "def show_images_batch(sample_batched):\n",
    "    images_batch = sample_batched['image']\n",
    "    batch_size = len(images_batch)\n",
    "    im_size = images_batch.size(2)\n",
    "    grid_border_size = 2\n",
    "\n",
    "    grid = utils.make_grid(images_batch)\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        plt.title('Batch from dataloader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched['image'].cuda().size())\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 3:\n",
    "        plt.figure()\n",
    "        show_images_batch(sample_batched)\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Using the csv files you created yesterday, create your own DataLoader using transforms that you think would be interesting or helpful. Here's a full list of transforms: https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "\n",
    "As a group, answer the following questions\n",
    "- Are the image labels accurate? That is to say, if it is labeled as having a flood, does it actually have a flood?\n",
    "- Do the transforms add enough variety to the dataset? Do they still look representative of the dataset?\n",
    "- Do the transforms distort from the labels? For example, if you use a random crop, is the thing that is being labeled still present in the image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a neural network\n",
    "We are now ready to train a CNN! We are going to start with a familiar step: we're going to split the dataset into a training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "test_split_ratio = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(transformed_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(test_split_ratio * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(transformed_dataset, batch_size=batch_size,\n",
    "                                           sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(transformed_dataset, batch_size=batch_size,\n",
    "                                                sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define our CNN architecture. It is going to be a combination of convolutional layers and fully connected layers. Here's the documentation of each of the layers: https://pytorch.org/docs/stable/nn.html \\*. \n",
    "\n",
    "\\* At this point, you might be wondering how I decided on this particular architecture. Why do we have 2 convolutional layers and not 3 (or 1 or 5)? The reality is that deciding on a neural network architecture is more of an art than a science. If you were to look up on the web for popular architectures, you would find thousands of architectures for all sorts of purposes. There are some general features that would make it more or less attractive for particular tasks, but really it's something you get a feel for rather than an exact science. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "try:\n",
    "    from cnn_finetune import make_model\n",
    "except:\n",
    "    !pip install cnn-finetune\n",
    "    from cnn_finetune import make_model\n",
    "\n",
    "net = make_model('resnet18', num_classes=2, pretrained=True).cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to define the loss function and the optimizer. You can think of the loss function as the amount of error you want to minimize, while the optimizer is the specific tool you're going to use to minimize that loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're going to do the actual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True # flag for some GPU optimizations\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs = data['image'].cuda()\n",
    "        labels = data['damage:flood/water'].cuda()\n",
    "        # casting int to long for loss calculation#\n",
    "        labels = labels.long()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 10 == 0:    # print every 10 mini-batches\n",
    "            print(f'[epoch {epoch+1}, batch {batch +1} ] average loss: {running_loss/10}')\n",
    "            running_loss = 0.0\n",
    "    # save the model\n",
    "    PATH = f'epoch_checkpoints/flood_checkpoint_epoch{epoch}.pth'\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Using your own dataset, set up your neural network so that it is ready to train. Leave it training overnight (it will take a *long* time for it to run). Make sure there are no obvious errors in your code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation\n",
    "\n",
    "Now that we have a fully trained CNN, we would like to see how well it performs on unseen data. Let's first take a sample of testing images and displaying their ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "single_iter = dataiter.next()\n",
    "images = single_iter['image']\n",
    "labels = single_iter['damage:flood/water']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print images\n",
    "imshow(utils.make_grid(images))\n",
    "print('GroundTruth: \\n', ' '.join('%5s' % labels[j].cpu() for j in range(16)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can load the saved model and make some predictions on the images above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "outputs = net(images.cuda())\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % predicted[j].cpu()\n",
    "                              for j in range(16)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can look at how the trained network performs on the whole testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images = data['image'].cuda()\n",
    "        labels = data['damage:flood/water'].cuda()\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can look at the performance of the model on each class of ```damage:flood/water: True``` and ```damage:flood/water: False```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(2))\n",
    "class_total = list(0. for i in range(2))\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images = data['image'].cuda()\n",
    "        labels = data['damage:flood/water'].cuda()\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(16):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        i, 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Display the performance of your neural network. Is the overall accuracy as good as you expected? What about the class accuracy? Does it perform better on one class than another? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
